---
weight: 2
title: "Regressão Linear Múltipla"
output: html_document
---

<script src="/math-code.js"></script> 
<script async 
src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> 
</script>

\newcommand{\bfbeta}{\hbox{$\boldsymbol \beta$}}
\newcommand{\bfY}{\textbf{Y}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
O modelo de regressão linear múltiplo é expresso pela função linear: $$y = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p + \varepsilon$$ 
em que:

$y$ é a variável resposta (variável dependente no modelo)
	
$x_1,...,x_p$ são as covariáveis (variáveis independentes ou explicativas),  supostamente independentes entre si;

$\beta_0,...,\beta_p$ são os coeficientes da regressão;

$\varepsilon$ é o erro aleatório.
	


Agora, considere uma amostra $y_1,...,y_n$ de $y$ em que cada $y_i$ está associado às $p$ variáveis explicativas, $x_i,x_{i1},...,x_{ip}$, $i=1,..,n$, assim pelo modelo $$y_i = \beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \varepsilon_i\,,$$
$i=1,..,n$ e $n>p$, que pode ser visto como um modelo de regressão linear amostral.

No modelo de regressão usual, os $\varepsilon_i$'s são variáveis aleatórias sujeitas as seguintes condições:

  * $E[\varepsilon_i] = 0$;
	* $Var(\varepsilon_i) = \sigma^2$;
	* $Cov(\varepsilon_i,\varepsilon_j) = 0 \,, \forall i \neq j\,, j=1,...,n$.

Note que, $x_{i1},...,x_{ip}$ são variáveis numéricas, não são variáveis aleatórias. No entanto, cada $y_i$ depende da quantidade aleatória $\varepsilon_i$ e portanto é uma variável aleatória. Assim, a média de $y_i$ é dada por: $$ E[y_i] = E[\beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \varepsilon_i] = \beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip}$$
e a variância é dada por: $$ Var(y_i) = Var(\beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \varepsilon_i) = \sigma^2, \ \ \  \forall i .$$

Usando a notação matricial, o modelo é dado por:

$$
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
=
\begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np} \\
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix}
+
\begin{pmatrix}
\varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n
\end{pmatrix}
\Rightarrow
\bfY = X  \beta+   \varepsilon
$$

A matriz $X$ de dimensão $n \times (p+1)$ e chamada de matriz de regressão quando $rank[X] = p+1$, e é chamada de matriz de delineamento quando $rank[X] = r < p+1$. A coluna de $1$'s na matriz refere-se ao intercepto, $\beta_0$. O vetor $Y$ de dimensão $n \times 1$ contém as variáveis $y_1,..,y_n$, $\beta$ é o vetor de dimensão $(p+1) \times 1$ dos coeficientes de regressão e $\varepsilon$ é o vetor de erros aleatórios de dimensão $n \times 1$.

Conseqüentemente, $$E[{ \bf Y}] = E[X{ \bf \beta}+{ \bf \varepsilon}]$$ e 
$$Var({ \bf Y}) = Var(X{ \bf \beta}+{ \bf \varepsilon}) = \sigma^2 I_n\,,$$ em que $I_n$ é a matriz identidade de dimensão $n \times n$.



**Estimação por mínimos quadrados**


O método de mínimos quadrados ($MMQ$) aplica-se somente aos parâmetros ${ \bf \beta} = (\beta_0,\beta_1,...,\beta_p)$, e é frequentemente aplicado em situações em que não se dispõe de mais especificações, além das que já foram feitas, sobre os erros. Este método consiste em estimar $\widehat{{ \bf \beta}} = (\widehat{\beta_0},\widehat{\beta_1},...,\widehat{\beta_p})$ por ${ \bf \beta} = (\beta_0,\beta_1,...,\beta_p)$ de modo que o vetor de valor esperado $E[{ \bf Y}] = X{ \bf \beta}$ esteja tão perto quanto possível do vetor de observações ${ \bf y}$ de ${ \bf Y}$. Ou seja, os estimadores de mínimos quadrados de $\beta_0,\beta_1,...,\beta_p$ devem minimizar a soma dos quadrados dos erros, dada por:

$$U({ \bf \beta}) = \displaystyle\sum_{i=1}^n\varepsilon_i^2 = \displaystyle\sum_{i=1}^n(y - \beta_0 - \beta_1x_{i1} - ... - \beta_px_{ip})^2 = 
{ \bf \varepsilon}^T{ \bf \varepsilon}$$

Nota que $U({ \bf \beta})$ pode ser expresso por:
$$U({ \bf \beta}) = { \bf Y}^T{ \bf Y} - { \bf \beta}^TX^T{ \bf Y} - { \bf Y}^TX{ \bf \beta} + { \bf \beta}^TX^TX{ \bf \beta} = { \bf Y}^T{ \bf Y} - 2{ \bf \beta}^TX^T{ \bf Y} + { \bf \beta}^TX^TX{ \bf \beta}$$

Derivando $U({ \bf \beta})$ com respeito a ${ \bf \beta}$ e igualando  a zero, temos:
$$\dfrac{\partial U({ \bf \beta})}{\partial { \bf \beta}} = - 2X^T{ \bf Y} + 2X^TX\widehat{{ \bf \beta}} = 0 \,,$$
que resulta em:
$$X^TX{ \bf \beta} = X^T{ \bf Y} \,,$$
denominadas equações normais.

Se $rank[X] = p+1$, $X^TX$ é positiva definida e, portanto,  inversível (não-singular). Assim, as equações normais possuem uma solução única dada por:
$$\widehat{{ \bf \beta}} = (X^TX)^{-1}X^T{ \bf Y} \,,$$
em que $\widehat{{ \bf \beta}}$ é o estimador de mínimos quadrados ($EMQ$) de $\beta$.

O modelo de regressão ajustado, correspondente ao vetor, ${ \bf Y}$, é dado por:
$$\widehat{{ \bf Y}} = X\widehat{{ \bf \beta}} = X(X^TX)^{-1}X^T{ \bf Y} = H{ \bf Y}\,.$$

A matriz $H = X(X^TX)^{-1}X^T$ de dimensão $n \times n$ é geralmente chamada de matriz chapéu. Essa matriz possui algumas propriedades importantes que são enunciadas no teorema a seguir.



Teorema $3.1$: Suponha que X é uma matriz $n \times (p+1)$ de $rank$ completo $p+1$. Então, 


* $H$ e ($I_n - H$) são simétricas e idempotente;

* $rank[I_n - H] = Tr[I_n - H] = n-(p+1) = n-p-1$;

* $HX = X$.
