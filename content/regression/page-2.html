---
weight: 2
title: "Regressão Linear Múltipla"
output: html_document
---



<script src="/math-code.js"></script>
<script async 
src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> 
</script>

<p>O modelo de regressão linear múltiplo é expresso pela função linear: <span class="math display">\[y = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p + \varepsilon\]</span>
em que:</p>
<p><span class="math inline">\(y\)</span> é a variável resposta (variável dependente no modelo)</p>
<p><span class="math inline">\(x_1,...,x_p\)</span> são as covariáveis (variáveis independentes ou explicativas), supostamente independentes entre si;</p>
<p><span class="math inline">\(\beta_0,...,\beta_p\)</span> são os coeficientes da regressão;</p>
<p><span class="math inline">\(\varepsilon\)</span> é o erro aleatório.</p>
<p>Agora, considere uma amostra <span class="math inline">\(y_1,...,y_n\)</span> de <span class="math inline">\(y\)</span> em que cada <span class="math inline">\(y_i\)</span> está associado às <span class="math inline">\(p\)</span> variáveis explicativas, <span class="math inline">\(x_i,x_{i1},...,x_{ip}\)</span>, <span class="math inline">\(i=1,..,n\)</span>, assim pelo modelo <span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \varepsilon_i\,,\]</span>
<span class="math inline">\(i=1,..,n\)</span> e <span class="math inline">\(n&gt;p\)</span>, que pode ser visto como um modelo de regressão linear amostral.</p>
<p>No modelo de regressão usual, os <span class="math inline">\(\varepsilon_i\)</span>’s são variáveis aleatórias sujeitas as seguintes condições:</p>
<ul>
<li><span class="math inline">\(E[\varepsilon_i] = 0\)</span>;
<ul>
<li><span class="math inline">\(Var(\varepsilon_i) = \sigma^2\)</span>;</li>
<li><span class="math inline">\(Cov(\varepsilon_i,\varepsilon_j) = 0 \,, \forall i \neq j\,, j=1,...,n\)</span>.</li>
</ul></li>
</ul>
<p>Note que, <span class="math inline">\(x_{i1},...,x_{ip}\)</span> são variáveis numéricas, não são variáveis aleatórias. No entanto, cada <span class="math inline">\(y_i\)</span> depende da quantidade aleatória <span class="math inline">\(\varepsilon_i\)</span> e portanto é uma variável aleatória. Assim, a média de <span class="math inline">\(y_i\)</span> é dada por: <span class="math display">\[ E[y_i] = E[\beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \varepsilon_i] = \beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip}\]</span>
e a variância é dada por: <span class="math display">\[ Var(y_i) = Var(\beta_0 + \beta_1x_{i1} + ... + \beta_px_{ip} + \varepsilon_i) = \sigma^2, \ \ \  \forall i .\]</span></p>
<p>Usando a notação matricial, o modelo é dado por:</p>
<p><span class="math display">\[
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \\
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix}
+
\begin{pmatrix}
\varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n
\end{pmatrix}
\Rightarrow
\textbf{Y}= X  \beta+   \varepsilon
\]</span></p>
<p>A matriz <span class="math inline">\(X\)</span> de dimensão <span class="math inline">\(n \times (p+1)\)</span> e chamada de matriz de regressão quando <span class="math inline">\(rank[X] = p+1\)</span>, e é chamada de matriz de delineamento quando <span class="math inline">\(rank[X] = r &lt; p+1\)</span>. A coluna de <span class="math inline">\(1\)</span>’s na matriz refere-se ao intercepto, <span class="math inline">\(\beta_0\)</span>. O vetor <span class="math inline">\(Y\)</span> de dimensão <span class="math inline">\(n \times 1\)</span> contém as variáveis <span class="math inline">\(y_1,..,y_n\)</span>, <span class="math inline">\(\beta\)</span> é o vetor de dimensão <span class="math inline">\((p+1) \times 1\)</span> dos coeficientes de regressão e <span class="math inline">\(\varepsilon\)</span> é o vetor de erros aleatórios de dimensão <span class="math inline">\(n \times 1\)</span>.</p>
<p>Conseqüentemente, <span class="math display">\[E[{ \bf Y}] = E[X{ \bf \beta}+{ \bf \varepsilon}]\]</span> e
<span class="math display">\[Var({ \bf Y}) = Var(X{ \bf \beta}+{ \bf \varepsilon}) = \sigma^2 I_n\,,\]</span> em que <span class="math inline">\(I_n\)</span> é a matriz identidade de dimensão <span class="math inline">\(n \times n\)</span>.</p>
<p><strong>Estimação por mínimos quadrados</strong></p>
<p>O método de mínimos quadrados (<span class="math inline">\(MMQ\)</span>) aplica-se somente aos parâmetros <span class="math inline">\({ \bf \beta} = (\beta_0,\beta_1,...,\beta_p)\)</span>, e é frequentemente aplicado em situações em que não se dispõe de mais especificações, além das que já foram feitas, sobre os erros. Este método consiste em estimar <span class="math inline">\(\widehat{{ \bf \beta}} = (\widehat{\beta_0},\widehat{\beta_1},...,\widehat{\beta_p})\)</span> por <span class="math inline">\({ \bf \beta} = (\beta_0,\beta_1,...,\beta_p)\)</span> de modo que o vetor de valor esperado <span class="math inline">\(E[{ \bf Y}] = X{ \bf \beta}\)</span> esteja tão perto quanto possível do vetor de observações <span class="math inline">\({ \bf y}\)</span> de <span class="math inline">\({ \bf Y}\)</span>. Ou seja, os estimadores de mínimos quadrados de <span class="math inline">\(\beta_0,\beta_1,...,\beta_p\)</span> devem minimizar a soma dos quadrados dos erros, dada por:</p>
<p><span class="math display">\[U({ \bf \beta}) = \displaystyle\sum_{i=1}^n\varepsilon_i^2 = \displaystyle\sum_{i=1}^n(y - \beta_0 - \beta_1x_{i1} - ... - \beta_px_{ip})^2 = 
{ \bf \varepsilon}^T{ \bf \varepsilon}\]</span></p>
<p>Nota que <span class="math inline">\(U({ \bf \beta})\)</span> pode ser expresso por:
<span class="math display">\[U({ \bf \beta}) = { \bf Y}^T{ \bf Y} - { \bf \beta}^TX^T{ \bf Y} - { \bf Y}^TX{ \bf \beta} + { \bf \beta}^TX^TX{ \bf \beta} = { \bf Y}^T{ \bf Y} - 2{ \bf \beta}^TX^T{ \bf Y} + { \bf \beta}^TX^TX{ \bf \beta}\]</span></p>
<p>Derivando <span class="math inline">\(U({ \bf \beta})\)</span> com respeito a <span class="math inline">\({ \bf \beta}\)</span> e igualando a zero, temos:
<span class="math display">\[\dfrac{\partial U({ \bf \beta})}{\partial { \bf \beta}} = - 2X^T{ \bf Y} + 2X^TX\widehat{{ \bf \beta}} = 0 \,,\]</span>
que resulta em:
<span class="math display">\[X^TX{ \bf \beta} = X^T{ \bf Y} \,,\]</span>
denominadas equações normais.</p>
<p>Se <span class="math inline">\(rank[X] = p+1\)</span>, <span class="math inline">\(X^TX\)</span> é positiva definida e, portanto, inversível (não-singular). Assim, as equações normais possuem uma solução única dada por:
<span class="math display">\[\widehat{{ \bf \beta}} = (X^TX)^{-1}X^T{ \bf Y} \,,\]</span>
em que <span class="math inline">\(\widehat{{ \bf \beta}}\)</span> é o estimador de mínimos quadrados (<span class="math inline">\(EMQ\)</span>) de <span class="math inline">\(\beta\)</span>.</p>
<p>O modelo de regressão ajustado, correspondente ao vetor, <span class="math inline">\({ \bf Y}\)</span>, é dado por:
<span class="math display">\[\widehat{{ \bf Y}} = X\widehat{{ \bf \beta}} = X(X^TX)^{-1}X^T{ \bf Y} = H{ \bf Y}\,.\]</span></p>
<p>A matriz <span class="math inline">\(H = X(X^TX)^{-1}X^T\)</span> de dimensão <span class="math inline">\(n \times n\)</span> é geralmente chamada de matriz chapéu. Essa matriz possui algumas propriedades importantes que são enunciadas no teorema a seguir.</p>
<p>Teorema <span class="math inline">\(3.1\)</span>: Suponha que X é uma matriz <span class="math inline">\(n \times (p+1)\)</span> de <span class="math inline">\(rank\)</span> completo <span class="math inline">\(p+1\)</span>. Então,</p>
<ul>
<li><p><span class="math inline">\(H\)</span> e (<span class="math inline">\(I_n - H\)</span>) são simétricas e idempotente;</p></li>
<li><p><span class="math inline">\(rank[I_n - H] = Tr[I_n - H] = n-(p+1) = n-p-1\)</span>;</p></li>
<li><p><span class="math inline">\(HX = X\)</span>.</p></li>
</ul>
